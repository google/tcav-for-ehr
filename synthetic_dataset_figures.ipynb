{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG6kJe_KPAgv"
      },
      "source": [
        "# Synthetic Dataset Figures\n",
        "\n",
        "This colab is intended to provide code for reproducing all synthetic-dataset related figures presented in the main paper.\n",
        "\n",
        "Note: This colab was built to support the data config used in the main paper. This means there are a couple of assumptions:\n",
        "* The dataset has been saved as a .pkl file (this is how the provided dataset generating script saves datasets).\n",
        "* There are two labels\n",
        "* There are two concepts, and their names are 'SINE0' and 'SINE1' (this is not a critical assumption - only certain color mappings are affected)\n",
        "\n",
        "If your dataset violates any of these assumptions, you will have to update the code to fit your use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "executionInfo": {
          "elapsed": 8274,
          "status": "ok",
          "timestamp": 1606065479774,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 300
        },
        "id": "UYI_fx0mLQ3_"
      },
      "outputs": [],
      "source": [
        "#@title Imports and helper functions\n",
        "\n",
        "import os\n",
        "import collections\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.cm import get_cmap\n",
        "from matplotlib import gridspec\n",
        "import sklearn.metrics\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n",
        "\n",
        "import dataset_utils\n",
        "import model_utils\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "SMALL_SIZE = 14\n",
        "MEDIUM_SIZE = 18\n",
        "LARGE_SIZE = 22\n",
        "\n",
        "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=LARGE_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=LARGE_SIZE)  # fontsize of the figure title\n",
        "\n",
        "# Helper functions\n",
        "\n",
        "def set_axis_style_bar(ax, labels=None, spineon=False):\n",
        "  \"\"\"Set certain axis parameters for bar plots.\"\"\"\n",
        "  ax.get_xaxis().set_tick_params(direction='out')\n",
        "  ax.get_yaxis().set_tick_params(direction='out')\n",
        "  if labels:\n",
        "    ax.set_xticks(np.arange(len(labels)))\n",
        "    ax.set_xticklabels(labels, fontsize=\"large\")\n",
        "    ax.set_xlim(-0.5, len(labels) - 0.5)\n",
        "  if not spineon:\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "  ax.yaxis.grid(True)\n",
        "  ax.xaxis.set_ticks_position('bottom')\n",
        "  ax.yaxis.set_ticks_position('left')\n",
        "\n",
        "def set_axis_style(ax, labels=None):\n",
        "  \"\"\"Set certain axis parameters for scatter/line plots.\"\"\"\n",
        "  ax.get_xaxis().set_tick_params(direction='out')\n",
        "  ax.get_yaxis().set_tick_params(direction='out')\n",
        "  if labels:\n",
        "    ax.set_xticks(np.arange(len(labels)))\n",
        "    ax.set_xticklabels(labels, fontsize=\"large\")\n",
        "    ax.set_xlim(-0.5, len(labels) - 0.5)\n",
        "  ax.yaxis.grid(True)\n",
        "  ax.hlines(0,0,100,linewidth=2,color=[0.5,0.5,0.5,0.8])\n",
        "  ax.spines['top'].set_visible(False)\n",
        "  ax.spines['right'].set_visible(False)\n",
        "  ax.xaxis.set_ticks_position('bottom')\n",
        "  ax.yaxis.set_ticks_position('left')\n",
        "\n",
        "def shift_array(xs, n):\n",
        "  \"\"\"Shifts a 1D array by n elements\"\"\"\n",
        "  if n == 0:\n",
        "    return xs\n",
        "  elif n \u003e 0:\n",
        "    return np.concatenate((np.full(n, np.nan), xs[:-n]))\n",
        "  else:\n",
        "    return np.concatenate((xs[-n:], np.full(-n, np.nan)))\n",
        "\n",
        "def plot_with_concept_mask_over_time(array, concept_mask, ax, color_pair, concept_name):\n",
        "  \"\"\"Plot mean array values and error over time, conditioned on concept presence and absence.\n",
        "  \n",
        "  Args:\n",
        "    array: Array of values of shape [batch_size, time, num_bootstraps]\n",
        "    concept_mask: Array of boolean values of shape [batch_size,] indicating the presence of a concept.\n",
        "    ax: Axis to plot on.\n",
        "    color_pair: A tuple pair of colors for plotting presence (second) and absence (first) lines.\n",
        "    concept_name: Name of concept for labeling plotted object.\n",
        "  \"\"\"\n",
        "  for presence in [False, True]:\n",
        "    masked_array = array[concept_mask==presence, :]\n",
        "    mean_masked = np.nanmean(masked_array, axis = 0)\n",
        "    mean = np.nanmean(mean_masked, axis=1)\n",
        "    std = np.nanstd(mean_masked, axis=1)\n",
        "    ax.plot(\n",
        "        list(range(masked_array.shape[1])),\n",
        "        mean,\n",
        "        label=f\"{concept_name} {'present' if presence else 'absent'}\",\n",
        "        color=color_pair[1] if presence else color_pair[0],\n",
        "        alpha=1 if presence else 0.6,\n",
        "        linestyle=\"-\" if presence else \"--\",\n",
        "        linewidth=2)\n",
        "    ax.fill_between(\n",
        "        list(range(masked_array.shape[1])),\n",
        "        mean-std, mean+std, \n",
        "        color=color_pair[1] if presence else color_pair[0],\n",
        "        alpha=0.5 if presence else 0.2)\n",
        "    ax.set_xlim(0, masked_array.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "executionInfo": {
          "elapsed": 53125,
          "status": "ok",
          "timestamp": 1606065532904,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 300
        },
        "id": "zTas7N4vUarV"
      },
      "outputs": [],
      "source": [
        "#@title Load data\n",
        "\n",
        "dataset_filename = \"Enter .pkl dataset filename here\" #@param {type: 'string'}\n",
        "dataset = dataset_utils.load_pickled_data(dataset_filename)\n",
        "\n",
        "# Dataset concept names have a specific order in the dataset that we must be aware of when plotting concept-related arrays.\n",
        "dataset_concept_names = [c[\"name\"] for c in dataset[\"config\"][\"concept_specs\"]]\n",
        "\n",
        "#@markdown Provide the synthetic_n_eval command line argument used for run_cs_tca.py below. We will collect the first n values of the loaded test dataset where n=synthetic_n_eval. \n",
        "#@markdown This is the dataset that was used to compute cs and tca metrics, and we will need these arrays for various figures.\n",
        "synthetic_n_eval = 100 #@param {type: 'integer'}\n",
        "\n",
        "eval_dataset_sequence = dataset[\"test_split\"][\"sequence\"][:, :synthetic_n_eval, :]\n",
        "eval_dataset_label = dataset[\"test_split\"][\"label\"][:, :synthetic_n_eval, :]\n",
        "eval_dataset_concept_changes = dataset[\"test_split\"][\"changes\"][:synthetic_n_eval, :]\n",
        "eval_dataset_concepts = dataset[\"test_split\"][\"concept\"][:synthetic_n_eval, :]\n",
        "eval_dataset_concept_sequences = dataset[\"test_split\"][\"concept_sequence\"][:, :synthetic_n_eval, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TP9HJlY1XzCV"
      },
      "outputs": [],
      "source": [
        "#@title Comparing Bootstrapped CAV metrics across strategies\n",
        "#@markdown There are various CAV building strategies discussed in the main paper, namely 't1_only,' 't0_to_t1,' and 't0_to_t1_diff'.  \n",
        "#@markdown In this section, we plot the accuracy of the classifiers trained to discriminate between concept and non-concept examples for each strategy.\n",
        "#@markdown Edit the cell and fill in directories to the cav output files (these should be the output_dir arguments of the create_cavs.py job) \n",
        "#@markdown for each of the strategies.\n",
        "\n",
        "#@markdown Note: Assumes metrics collected are same for all strategies. If this is not the case, the code will need to be updated.\n",
        "\n",
        "strategy_and_cav_dir_pairs = [\n",
        "  (\"t1 only\", \"cav directory here\"),\n",
        "  (\"t0 to t1\", \"cav directory here\"),\n",
        "  (\"t0 to t1 diff\", \"cav directory here\")\n",
        "]\n",
        "metrics_by_strategy = {strat: dataset_utils.load_pickled_data(os.path.join(dir, \"cav_metrics.pkl\")) for strat, dir in strategy_and_cav_dir_pairs}\n",
        "\n",
        "# get all relevant strategy, metric, concept, and layers from metrics objects.\n",
        "strategy_names = set()\n",
        "metric_names = set()\n",
        "concept_names = set()\n",
        "layers = set()\n",
        "\n",
        "for strat, metrics in metrics_by_strategy.items():\n",
        "  strategy_names.add(strat)\n",
        "  for concept_name in metrics:\n",
        "    concept_names.add(concept_name)\n",
        "    for layer in metrics[concept_name]:\n",
        "      layers.add(layer)\n",
        "      for metric_name in metrics[concept_name][layer]:\n",
        "        metric_names.add(metric_name)\n",
        "\n",
        "strategy_names = list(strategy_names)\n",
        "metric_names = list(metric_names)\n",
        "concept_names = list(concept_names)\n",
        "layers = list(layers)\n",
        "\n",
        "print(\"All strategies:\", strategy_names)\n",
        "print(\"All metrics:\", metric_names)\n",
        "print(\"All concepts:\", concept_names)\n",
        "print(\"All layers:\", layers)\n",
        "\n",
        "# Create dataframe with reported CAV metrics                              \n",
        "metric_results_rows = []               \n",
        "for strat, metrics in metrics_by_strategy.items():\n",
        "  for concept_name in metrics:\n",
        "    for layer in metrics[concept_name]:\n",
        "      metric_results_row = {\n",
        "          \"strategy\": strat,\n",
        "          \"concept\": concept_name,\n",
        "          \"layer\": layer\n",
        "      }\n",
        "      for metric_name in metrics[concept_name][layer]:\n",
        "        metric_result = metrics[concept_name][layer][metric_name]\n",
        "        pvalue, baseline_score, baseline_score_std, permuted_scores = metric_result\n",
        "        worst_num_extremes = np.sum(np.array(permuted_scores) \u003e= (baseline_score - baseline_score_std))\n",
        "        worst_pvalue = (worst_num_extremes + 1) / float(len(permuted_scores) + 1)\n",
        "        metric_results_row.update({\n",
        "            f\"{metric_name}_avg\": baseline_score,\n",
        "            f\"{metric_name}_std\": baseline_score_std,\n",
        "            f\"{metric_name}_pvalue\": pvalue,\n",
        "            f\"{metric_name}_pvalue_worst\": worst_pvalue\n",
        "        })\n",
        "      metric_results_rows.append(metric_results_row)\n",
        "metrics_results_df = pd.DataFrame(metric_results_rows)\n",
        "\n",
        "# Plot held out bootstrap CAV classifier performance\n",
        "plt.style.use(\"classic\")\n",
        "name = \"Blues\"\n",
        "cmap = get_cmap(name)\n",
        "idx_col = np.array(range(len(strategy_and_cav_dir_pairs))) / len(strategy_and_cav_dir_pairs)\n",
        "colors=[]\n",
        "for ic in idx_col:\n",
        "  colors.append(cmap(ic))\n",
        "alphas = [0.9, 0.9]\n",
        "\n",
        "bar_width = 1 / (len(strategy_and_cav_dir_pairs)+1)\n",
        "plt.rcParams.update({'font.size': 14})\n",
        "strategy_names = list(metrics_by_strategy.keys())\n",
        "\n",
        "fig, ax = plt.subplots(1, len(concept_names), figsize=(13, 4), dpi=300, sharey=True)\n",
        "for strat_idx, strat in enumerate(strategy_names):\n",
        "  for concept_idx, concept_name in enumerate(concept_names):\n",
        "    results_df = (metrics_results_df[(metrics_results_df[\"strategy\"] == strat) \u0026 (metrics_results_df[\"concept\"] == concept_name)]\n",
        "                  .sort_values(\"layer\"))\n",
        "    ax[concept_idx].bar(\n",
        "        np.array(range(len(layers))) + bar_width * strat_idx,\n",
        "        results_df[\"balanced_accuracy_avg\"]*100,\n",
        "        width=bar_width,\n",
        "        color=colors[strat_idx],\n",
        "        alpha=alphas[1])\n",
        "    ax[concept_idx].errorbar(\n",
        "        np.array(range(len(layers))) + bar_width * strat_idx,\n",
        "        results_df[\"balanced_accuracy_avg\"].to_numpy()*100,\n",
        "        yerr=results_df[\"balanced_accuracy_std\"].to_numpy()*100,\n",
        "        fmt=\"none\",\n",
        "        label=\"_nolegend_\",\n",
        "        ecolor=[0, 0, 0],\n",
        "        elinewidth=2)\n",
        "    ax[concept_idx].set_ylim([0, 105])\n",
        "    ax[concept_idx].set_xticks(\n",
        "        np.array(range(len(layers))) +\n",
        "        (len(strategy_names)+1) * bar_width / len(strategy_names))\n",
        "    ax[concept_idx].set_xticklabels([f\"layer {layer}\" for layer in layers],\n",
        "                                    fontsize=18, fontweight='bold')\n",
        "    set_axis_style_bar(ax[concept_idx])\n",
        "    ax[concept_idx].set_title(f\"Concept: {concept_name}\", fontsize=\"large\", wrap=True, fontweight='bold')\n",
        "ax[0].set_ylabel(\"Bootstrapped accuracy [%]\", fontsize=\"large\", wrap=True, fontweight='bold')\n",
        "\n",
        "legend = ax[1].legend(strategy_names, loc = \"upper right\", bbox_to_anchor=(2, 1), fontsize=18, frameon=False, title=\"CAV strategy\")\n",
        "legend.get_title().set_fontsize(\"20\")\n",
        "legend.get_title().set_fontweight(\"bold\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dsnsRmcvjWVr"
      },
      "outputs": [],
      "source": [
        "#@title Comparing CAV metrics on unseen test data across strategies\n",
        "#@markdown Here we recompute accuracy and rocauc for each strategy on an unseen cohort of test data elements. We use a different slice of the test\n",
        "#@markdown dataset than is used for final cs and tca evaluation. This code uses the same directories provided in the cell above. Provide\n",
        "#@markdown a path to a trained model checkpoint and the number of unseen data to compute metrics for below.\n",
        "\n",
        "#@markdown Note: Assumes metrics collected are same for all strategies. If this is not the case, the code will need to be updated.\n",
        "\n",
        "\n",
        "# Compute performance on unseen test data\n",
        "model_checkpoint_path = \"Enter model checkpoint path here. It should end with /tfhub\" #@param {type: 'string'}\n",
        "num_unseen_data_to_test = 100 #@param {type: 'integer'}\n",
        "\n",
        "unseen_test_sequence = dataset[\"test_split\"][\"sequence\"][:, synthetic_n_eval:synthetic_n_eval+num_unseen_data_to_test, :]\n",
        "unseen_test_label = dataset[\"test_split\"][\"label\"][:, synthetic_n_eval:synthetic_n_eval+num_unseen_data_to_test, :]\n",
        "unseen_test_concept_sequences = dataset[\"test_split\"][\"concept_sequence\"][:, synthetic_n_eval:synthetic_n_eval+num_unseen_data_to_test, :]\n",
        "unseen_test_output = model_utils.unroll_model_per_step(\n",
        "    model_checkpoint_path,\n",
        "    unseen_test_sequence,\n",
        "    unseen_test_label)\n",
        "\n",
        "models_by_strategy = {strat: dataset_utils.load_pickled_data(os.path.join(dir, \"cav_classifiers.pkl\")) for strat, dir in strategy_and_cav_dir_pairs}\n",
        "\n",
        "unseen_metrics_results_rows = []\n",
        "ys_all = np.reshape(unseen_test_concept_sequences, [-1, unseen_test_concept_sequences.shape[-1]])\n",
        "for strat, models in models_by_strategy.items():\n",
        "  for dataset_concept_idx, concept_name in enumerate(dataset_concept_names):\n",
        "    ys = ys_all[:, dataset_concept_idx:dataset_concept_idx+1]\n",
        "    for layer, acts in enumerate(unseen_test_output[\"activations\"]):\n",
        "      xs = np.reshape(acts, [-1, acts.shape[-1]])\n",
        "      balanced_accuracies = []\n",
        "      rocaucs = []\n",
        "      for trained_model in models[concept_name][layer]:\n",
        "        preds = trained_model.predict(xs)\n",
        "        balanced_accuracy = sklearn.metrics.balanced_accuracy_score(ys, preds)\n",
        "        balanced_accuracies.append(balanced_accuracy)\n",
        "        try:\n",
        "          probs = trained_model.predict_proba(xs)[:, 1]\n",
        "        except AttributeError:\n",
        "          probs = trained_model.decision_function(xs)\n",
        "        rocauc = sklearn.metrics.roc_auc_score(ys, probs)\n",
        "        rocaucs.append(rocauc)\n",
        "      unseen_metrics_results_rows.append({\n",
        "          \"strategy\": strat,\n",
        "          \"concept\": concept_name,\n",
        "          \"layer\": layer,\n",
        "          \"balanced_accuracy_avg\": np.mean(balanced_accuracies),\n",
        "          \"balanced_accuracy_std\": np.std(balanced_accuracies),\n",
        "          \"rocauc_avg\": np.mean(rocaucs),\n",
        "          \"rocauc_std\": np.std(rocaucs)\n",
        "      })\n",
        "unseen_metrics_results_df = pd.DataFrame(unseen_metrics_results_rows)\n",
        "\n",
        "# Plot held out bootstrap CAV classifier performance\n",
        "plt.style.use(\"classic\")\n",
        "name = \"Blues\"\n",
        "cmap = get_cmap(name)\n",
        "idx_col = np.array(range(len(strategy_and_cav_dir_pairs))) / len(strategy_and_cav_dir_pairs)\n",
        "colors=[]\n",
        "for ic in idx_col:\n",
        "  colors.append(cmap(ic))\n",
        "alphas = [0.9, 0.9]\n",
        "\n",
        "bar_width = 1 / (len(strategy_and_cav_dir_pairs)+1)\n",
        "plt.rcParams.update({'font.size': 14})\n",
        "strategy_names = list(metrics_by_strategy.keys())\n",
        "\n",
        "fig, ax = plt.subplots(1, len(concept_names), figsize=(13, 4), dpi=300, sharey=True)\n",
        "for strat_idx, strat in enumerate(strategy_names):\n",
        "  for concept_idx, concept_name in enumerate(concept_names):\n",
        "    results_df = (unseen_metrics_results_df[(unseen_metrics_results_df[\"strategy\"] == strat) \u0026 (unseen_metrics_results_df[\"concept\"] == concept_name)]\n",
        "                  .sort_values(\"layer\"))\n",
        "    ax[concept_idx].bar(\n",
        "        np.array(range(len(layers))) + bar_width * strat_idx,\n",
        "        results_df[\"balanced_accuracy_avg\"]*100,\n",
        "        width=bar_width,\n",
        "        color=colors[strat_idx],\n",
        "        alpha=alphas[1])\n",
        "    ax[concept_idx].errorbar(\n",
        "        np.array(range(len(layers))) + bar_width * strat_idx,\n",
        "        results_df[\"balanced_accuracy_avg\"].to_numpy()*100,\n",
        "        yerr=results_df[\"balanced_accuracy_std\"].to_numpy()*100,\n",
        "        fmt=\"none\",\n",
        "        label=\"_nolegend_\",\n",
        "        ecolor=[0, 0, 0],\n",
        "        elinewidth=2)\n",
        "    ax[concept_idx].set_ylim([0, 105])\n",
        "    ax[concept_idx].set_xticks(\n",
        "        np.array(range(len(layers))) +\n",
        "        (len(strategy_names)+1) * bar_width / len(strategy_names))\n",
        "    ax[concept_idx].set_xticklabels([f\"layer {layer}\" for layer in layers],\n",
        "                                    fontsize=18, fontweight='bold')\n",
        "    set_axis_style_bar(ax[concept_idx])\n",
        "    ax[concept_idx].set_title(f\"Concept: {concept_name}\", fontsize=\"large\", wrap=True, fontweight='bold')\n",
        "ax[0].set_ylabel(\"Bootstrapped accuracy [%]\", fontsize=\"large\", wrap=True, fontweight='bold')\n",
        "\n",
        "legend = ax[1].legend(strategy_names, loc = \"upper right\", bbox_to_anchor=(2, 1), fontsize=18, frameon=False, title=\"CAV strategy\")\n",
        "legend.get_title().set_fontsize(\"20\")\n",
        "legend.get_title().set_fontweight(\"bold\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qxeqNfQMT7DH"
      },
      "outputs": [],
      "source": [
        "#@title Load CS and tCA results\n",
        "\n",
        "def load_cs_tca_results(cs_tca_directory):\n",
        "  cs_results = collections.defaultdict(dict)\n",
        "  tca_results = collections.defaultdict(dict)\n",
        "  for target_subdir in os.listdir(cs_tca_directory):\n",
        "    target = int(target_subdir.replace(\"target=\", \"\"))\n",
        "    for concept_subdir in os.listdir(os.path.join(cs_tca_directory, target_subdir)):\n",
        "      concept_name = concept_subdir.replace(\"concept=\", \"\")\n",
        "      cs_results[target][concept_name], tca_results[target][concept_name] = {}, {}\n",
        "      for layer_subdir in os.listdir(os.path.join(cs_tca_directory, target_subdir, concept_subdir)):\n",
        "        full_dir = os.path.join(cs_tca_directory, target_subdir, concept_subdir, layer_subdir)\n",
        "        layer = int(layer_subdir.replace(\"layer=\", \"\"))\n",
        "        cs_results[target][concept_name][layer] = dataset_utils.load_pickled_data(os.path.join(full_dir, \"cs_results.pkl\"))\n",
        "        tca_results[target][concept_name][layer] = dataset_utils.load_pickled_data(os.path.join(full_dir, \"tca_results.pkl\"))\n",
        "  return cs_results, tca_results\n",
        "\n",
        "cs_tca_directory = \"Enter cs tca directory here. It should be the same as the output_dir command line argument used for run_cs_tca.py\" #@param {type: 'string'}\n",
        "cs_results, tca_results = load_cs_tca_results(cs_tca_directory)\n",
        "\n",
        "targets = set()\n",
        "concept_names = set()\n",
        "layers = set()\n",
        "\n",
        "for target in cs_results:\n",
        "  targets.add(target)\n",
        "  for concept_name in cs_results[target]:\n",
        "    concept_names.add(concept_name)\n",
        "    for layer in cs_results[target][concept_name]:\n",
        "      layers.add(layer)\n",
        "\n",
        "targets = list(targets)\n",
        "concept_names = list(concept_names)\n",
        "layers = list(layers)\n",
        "\n",
        "print(\"All targets:\", targets)\n",
        "print(\"All concepts:\", concept_names)\n",
        "print(\"All layers:\", layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-7OzYri2UAxH"
      },
      "outputs": [],
      "source": [
        "#@title Global CS scores\n",
        "\n",
        "name = \"tab20c\"\n",
        "cmap = get_cmap(name)\n",
        "cols = cmap.colors\n",
        "mycolors = {\"SINE0\": cols[13], \"SINE1\": cols[9]}\n",
        "\n",
        "fig = plt.figure(figsize=(12, 8), constrained_layout=False, dpi=300)\n",
        "outer = fig.add_gridspec(1, 2, wspace=0.4, hspace=0.5)\n",
        "alpha = [0.4, 0.9] \n",
        "ax = None\n",
        "for target in targets:\n",
        "  subplts = outer[target].subgridspec(len(layers), 1)\n",
        "  plt.rcParams.update({'font.size': 14})\n",
        "  barWidth=0.3\n",
        "  ax_count = 0\n",
        "  for layer in layers:\n",
        "    ax = fig.add_subplot(subplts[ax_count], sharey=ax)\n",
        "    xvals_bars = range(1, len(concept_names) + 1)\n",
        "    for concept_idx, concept_name in enumerate(concept_names):\n",
        "      eval_dataset_concept_sequence = eval_dataset_concept_sequences[:, :, dataset_concept_names.index(concept_name)]\n",
        "      for presence in [0, 1]:\n",
        "        pres_label = \" present\" if presence else \" absent\"\n",
        "        data = np.nanmean(cs_results[target][concept_name][layer][\"bootstrap_CS\"][eval_dataset_concept_sequence==presence],axis=0)\n",
        "        ax.bar(xvals_bars[concept_idx] + barWidth * presence,\n",
        "              np.nanmean(data), width=barWidth, color=mycolors[concept_name],\n",
        "              alpha=alpha[presence], label=concept_name + pres_label)\n",
        "        ax.errorbar(xvals_bars[concept_idx] + barWidth * presence,\n",
        "                    np.nanmean(data),\n",
        "                    yerr=np.nanstd(data),\n",
        "                    fmt='none', ecolor=[0,0,0], elinewidth=2)\n",
        "    if ax_count == 0:\n",
        "      plt.title(f\"target y_{target}\", fontsize=24, fontweight=\"bold\")\n",
        "    ax.set_ylabel(\"Layer \"+str(layer),fontsize=24, fontweight='bold')\n",
        "    xtick_labels = concept_names\n",
        "    ax.set_xticks(np.array(xvals_bars) + barWidth/2)\n",
        "    ax.set_xticklabels(\"\", fontsize=18, fontweight='bold')\n",
        "    if ax_count == len(layers)-1:\n",
        "      ax.set_xticklabels(xtick_labels, fontsize=18, fontweight='bold')\n",
        "    ax_count += 1\n",
        "\n",
        "legend = ax.legend(loc=\"upper left\", bbox_to_anchor=(1.05, 1.1),title=\"Score\", fontsize=18, frameon=True)\n",
        "legend.get_title().set_fontsize(\"20\")\n",
        "legend.get_title().set_fontweight(\"bold\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KwWc31BlXzWb"
      },
      "outputs": [],
      "source": [
        "#@title Global CS scores as time series\n",
        "\n",
        "desired_changepoint = 50\n",
        "t0, t1 = 0, 100\n",
        "layer = 2\n",
        "\n",
        "CS_target_arrays = {}\n",
        "for target in targets:\n",
        "  concept_arrays = []\n",
        "  for concept_name in concept_names:\n",
        "    cs_array = cs_results[target][concept_name][layer][\"bootstrap_CS\"]\n",
        "    cps = eval_dataset_concept_changes[:, dataset_concept_names.index(concept_name)]\n",
        "    shifted_cs_array = []\n",
        "    for i in range(len(cps)):\n",
        "      cp = cps[i]\n",
        "      shift_x = desired_changepoint - cp\n",
        "      shifted_boots_results = []\n",
        "      num_bootstraps = cs_array.shape[-1]\n",
        "      for bootstrap in range(num_bootstraps):\n",
        "        cs_boot = cs_array[:, i, bootstrap]\n",
        "        shifted_cs = np.reshape(shift_array(cs_boot.tolist(), shift_x), [1, -1])\n",
        "        shifted_boots_results.append(shifted_cs)\n",
        "      shifted_boots_results = np.reshape(np.array(shifted_boots_results).T, (1, cs_boot.shape[0], num_bootstraps))\n",
        "      shifted_cs_array.append(shifted_boots_results)\n",
        "    shifted_cs_array = np.concatenate(shifted_cs_array, axis=0)\n",
        "    windowed_cs_array = shifted_cs_array[:, t0:t1]\n",
        "    concept_arrays.append(windowed_cs_array)\n",
        "  CS_target_arrays[target] = concept_arrays\n",
        "\n",
        "concept_colors = {\n",
        "    \"SINE0\": (cols[13], cols[13]),\n",
        "    \"SINE1\": (cols[9], cols[9])\n",
        "}\n",
        "\n",
        "fig = plt.figure(figsize=(12, 4), dpi=300)\n",
        "gs = gridspec.GridSpec(nrows=1, ncols=2, figure=fig)\n",
        "gs.update(wspace=0.4, hspace=0.5)\n",
        "for target in targets:\n",
        "  if target == 0:\n",
        "    ax = fig.add_subplot(gs[0, target])\n",
        "    ax1 = ax\n",
        "  else:\n",
        "    ax = fig.add_subplot(gs[0,target],sharey=ax1)\n",
        "  set_axis_style(ax)\n",
        "  ax.set_xlabel(\"Time\", fontsize=\"x-large\", wrap=True, fontweight='bold')\n",
        "  ax.set_ylabel(\"CS\" if not target else None, fontsize=\"x-large\", wrap=True, fontweight='bold')\n",
        "  for concept_idx, concept_name in enumerate(concept_names):\n",
        "    target_array = CS_target_arrays[target][concept_idx]\n",
        "    plot_with_concept_mask_over_time(\n",
        "        array=target_array, \n",
        "        concept_mask=eval_dataset_concepts[:, dataset_concept_names.index(concept_name)], \n",
        "        ax=ax, \n",
        "        color_pair=concept_colors[concept_name],\n",
        "        concept_name=concept_name)\n",
        "  ax.set_title(f\"Target y_{target}\", fontweight='bold')\n",
        "legend = ax.legend(loc=\"upper right\", title=\"Score\", bbox_to_anchor=(1.8,1),frameon=True)\n",
        "legend.get_title().set_fontsize(\"20\")\n",
        "legend.get_title().set_fontweight(\"bold\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9FtjVrm66d0w"
      },
      "outputs": [],
      "source": [
        "#@title Compute model performance on eval dataset for error type analysis\n",
        "#@markdown In order to analyze concept-based scores for each error type (true positives, false positives, etc.),\n",
        "#@markdown we need to compute model predictions for all eval dataset examples.\n",
        "\n",
        "model_checkpoint_path = \"Enter model checkpoint path here. It should end with /tfhub\"  #@param {type: 'string'}\n",
        "\n",
        "eval_dataset_outputs = model_utils.unroll_model(\n",
        "    model_checkpoint_path,\n",
        "    eval_dataset_sequence,\n",
        "    eval_dataset_label)\n",
        "eval_dataset_predictions = eval_dataset_outputs[\"predictions\"]\n",
        "eval_dataset_probs = eval_dataset_outputs[\"probs\"]\n",
        "eval_dataset_acc = sklearn.metrics.accuracy_score(\n",
        "    eval_dataset_label.reshape(-1), eval_dataset_predictions.reshape(-1))*100\n",
        "eval_dataset_prauc = sklearn.metrics.average_precision_score(\n",
        "    eval_dataset_label.reshape(-1), eval_dataset_predictions.reshape(-1))\n",
        "eval_dataset_rocauc = sklearn.metrics.roc_auc_score(\n",
        "    eval_dataset_label.reshape(-1), eval_dataset_predictions.reshape(-1))\n",
        "print(\"eval dataset accuracy: %.2f\" % eval_dataset_acc)\n",
        "print(\"eval dataset prauc: %.4f\" % eval_dataset_prauc)\n",
        "print(\"eval dataset rocauc: %.4f\" % eval_dataset_rocauc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dP8IJzH5SvRp"
      },
      "outputs": [],
      "source": [
        "#@title Global CS scores as time series, by error type\n",
        "concept_colors = {\n",
        "    \"SINE0\": (cols[13], cols[13]),\n",
        "    \"SINE1\": (cols[9], cols[9])\n",
        "}\n",
        "\n",
        "gs_kw = dict(wspace=0.4, hspace=0.5)\n",
        "fig, ax = plt.subplots(ncols=2, nrows=4, gridspec_kw=gs_kw, figsize=(10,16), sharey=True, sharex=True, dpi=300)\n",
        "\n",
        "for target in targets:\n",
        "  pos_pred_mask = (np.max(eval_dataset_predictions[:, :, target], axis=0) == 1)\n",
        "  pos_label_mask = (np.max(eval_dataset_label[:, :, target], axis=0) == 1)\n",
        "  for concept_idx in [0, 1]:\n",
        "    concept_name = concept_names[concept_idx]\n",
        "    # TP\n",
        "    target_array = CS_target_arrays[target][concept_idx]\n",
        "    filtered_target_array = target_array[pos_pred_mask * pos_label_mask, :]\n",
        "    filtered_concept_mask = eval_dataset_concepts[:, dataset_concept_names.index(concept_name)][pos_pred_mask * pos_label_mask]\n",
        "    plot_with_concept_mask_over_time(\n",
        "        array=filtered_target_array,\n",
        "        concept_mask=filtered_concept_mask, \n",
        "        ax=ax[0][target], \n",
        "        color_pair=concept_colors[concept_name],\n",
        "        concept_name=concept_name)\n",
        "    set_axis_style(ax[0][target])\n",
        "    ax[0][target].set_xlabel(\"Time\", fontsize=\"x-large\", wrap=True, fontweight='bold')\n",
        "    ax[0][target].set_ylabel(\"CS\" if not target else None, fontsize=\"x-large\", wrap=True, fontweight='bold')\n",
        "    ax[0][target].set_title(f\"Target y_{target} TP, n={np.size(filtered_concept_mask)}\", fontweight='bold')\n",
        "\n",
        "    # FP\n",
        "    filtered_target_array = target_array[pos_pred_mask * np.logical_not(pos_label_mask), :]\n",
        "    filtered_concept_mask = eval_dataset_concepts[:, dataset_concept_names.index(concept_name)][pos_pred_mask * np.logical_not(pos_label_mask)]\n",
        "    plot_with_concept_mask_over_time(\n",
        "        array=filtered_target_array, \n",
        "        concept_mask=filtered_concept_mask, \n",
        "        ax=ax[1][target], \n",
        "        color_pair=concept_colors[concept_name],\n",
        "        concept_name=concept_name)\n",
        "    set_axis_style(ax[1][target])\n",
        "    ax[1][target].set_xlabel(\"Time\", fontsize=\"x-large\", wrap=True, fontweight='bold')\n",
        "    ax[1][target].set_ylabel(\"CS\" if not target else None, fontsize=\"x-large\", wrap=True, fontweight='bold') \n",
        "    ax[1][target].set_title(f\"Target y_{target} FP, n={np.size(filtered_concept_mask)}\", fontweight='bold')\n",
        "\n",
        "    # TN\n",
        "    filtered_target_array = target_array[np.logical_not(pos_pred_mask) * np.logical_not(pos_label_mask), :]\n",
        "    filtered_concept_mask = eval_dataset_concepts[:, dataset_concept_names.index(concept_name)][np.logical_not(pos_pred_mask) * np.logical_not(pos_label_mask)]\n",
        "    plot_with_concept_mask_over_time(\n",
        "        array=filtered_target_array, \n",
        "        concept_mask=filtered_concept_mask, \n",
        "        ax=ax[2][target], \n",
        "        color_pair=concept_colors[concept_name],\n",
        "        concept_name=concept_name)\n",
        "    set_axis_style(ax[2][target])\n",
        "    ax[2][target].set_xlabel(\"Time\", fontsize=\"x-large\", wrap=True, fontweight='bold')\n",
        "    ax[2][target].set_ylabel(\"CS\" if not target else None, fontsize=\"x-large\", wrap=True, fontweight='bold')\n",
        "    ax[2][target].set_title(f\"Target y_{target} TN, n={np.size(filtered_concept_mask)}\", fontweight='bold')\n",
        "\n",
        "    # FN\n",
        "    filtered_target_array = target_array[np.logical_not(pos_pred_mask) * pos_label_mask, :]\n",
        "    filtered_concept_mask = eval_dataset_concepts[:, dataset_concept_names.index(concept_name)][np.logical_not(pos_pred_mask) * pos_label_mask]\n",
        "    plot_with_concept_mask_over_time(\n",
        "        array=filtered_target_array, \n",
        "        concept_mask=filtered_concept_mask, \n",
        "        ax=ax[3][target], \n",
        "        color_pair=concept_colors[concept_name],\n",
        "        concept_name=concept_name)\n",
        "    set_axis_style(ax[3][target])\n",
        "    ax[3][target].set_xlabel(\"Time\", fontsize=\"x-large\", wrap=True, fontweight='bold')\n",
        "    ax[3][target].set_ylabel(\"CS\" if not target else None, fontsize=\"x-large\", wrap=True, fontweight='bold')\n",
        "    ax[3][target].set_title(f\"Target y_{target} FN, n={np.size(filtered_concept_mask)}\", fontweight='bold')\n",
        "\n",
        "legend = ax[0][1].legend(loc=\"upper right\", title=\"Score\", bbox_to_anchor=(1.8, 1), frameon=True)\n",
        "legend.get_title().set_fontsize(\"20\")\n",
        "legend.get_title().set_fontweight(\"bold\")\n",
        "   \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tcD6-0276Ini"
      },
      "outputs": [],
      "source": [
        "#@title Global tCA Scores\n",
        "\n",
        "name = \"tab20c\"\n",
        "cmap = get_cmap(name)\n",
        "cols = cmap.colors\n",
        "mycolors = {\"SINE0\": cols[13], \"SINE1\": cols[9]}\n",
        "\n",
        "fig = plt.figure(figsize=(6, 7), constrained_layout=False, dpi=300)\n",
        "outer = fig.add_gridspec(1, 1, wspace=0.3)\n",
        "alpha = [0.4, 0.9] \n",
        "\n",
        "subplts = outer[0].subgridspec(len(layers), 1)\n",
        "plt.rcParams.update({'font.size': 14})\n",
        "barWidth=0.3\n",
        "ax_count = 0\n",
        "ax = None\n",
        "for layer in layers:\n",
        "  ax = fig.add_subplot(subplts[ax_count], sharey=ax)\n",
        "  xvals_bars = range(1, len(concept_names) + 1)\n",
        "  for concept_idx, concept_name in enumerate(concept_names):\n",
        "    eval_dataset_concept_sequence = eval_dataset_concept_sequences[:, :, dataset_concept_names.index(concept_name)]\n",
        "    for presence in [0, 1]:\n",
        "      pres_label = \" present\" if presence else \" absent\"\n",
        "      data = np.nanmean(tca_results[target][concept_name][layer][\"bootstrap_tCA\"][eval_dataset_concept_sequence==presence],axis=0)\n",
        "      ax.bar(xvals_bars[concept_idx] + barWidth * presence,\n",
        "            np.nanmean(data), width=barWidth, color=mycolors[concept_name],\n",
        "            alpha=alpha[presence], label=concept_name + pres_label)\n",
        "      ax.errorbar(xvals_bars[concept_idx] + barWidth * presence,\n",
        "                  np.nanmean(data),\n",
        "                  yerr=np.nanstd(data),\n",
        "                  fmt='none', ecolor=[0,0,0], elinewidth=2)\n",
        "  ax.set_ylabel(\"Layer \"+str(layer),fontsize=24, fontweight='bold')\n",
        "  xtick_labels = concept_names\n",
        "  ax.set_xticks(np.array(xvals_bars) + barWidth/2)\n",
        "  ax.set_xticklabels(\"\", fontsize=18, fontweight='bold')\n",
        "  if ax_count == len(layers)-1:\n",
        "    ax.set_xticklabels(xtick_labels, fontsize=18, fontweight='bold')\n",
        "  ax_count += 1\n",
        "\n",
        "legend = ax.legend(loc=\"upper left\", bbox_to_anchor=(1.05, 1.1), title=\"Score\", fontsize=18, frameon=True)\n",
        "legend.get_title().set_fontsize(\"20\")\n",
        "legend.get_title().set_fontweight(\"bold\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "54DfTUZc0JZW"
      },
      "outputs": [],
      "source": [
        "#@title Global tCA scores as time series\n",
        "\n",
        "desired_changepoint = 50\n",
        "t0, t1 = 0, 100\n",
        "layer = 2\n",
        "dummy_target = 0\n",
        "\n",
        "concept_arrays = []\n",
        "for concept_name in concept_names:\n",
        "  tca_array = tca_results[dummy_target][concept_name][layer][\"bootstrap_tCA\"]\n",
        "  cps = eval_dataset_concept_changes[:, dataset_concept_names.index(concept_name)]\n",
        "  shifted_tca_array = []\n",
        "  for i in range(len(cps)):\n",
        "    cp = cps[i]\n",
        "    shift_x = desired_changepoint - cp\n",
        "    shifted_boots_results = []\n",
        "    num_bootstraps = tca_array.shape[-1]\n",
        "    for bootstrap in range(num_bootstraps):\n",
        "      tca_boot = tca_array[:, i, bootstrap]\n",
        "      shifted_tca = np.reshape(shift_array(tca_boot.tolist(), shift_x), [1, -1])\n",
        "      shifted_boots_results.append(shifted_tca)\n",
        "    shifted_boots_results = np.reshape(np.array(shifted_boots_results).T, (1, tca_boot.shape[0], num_bootstraps))\n",
        "    shifted_tca_array.append(shifted_boots_results)\n",
        "  shifted_tca_array = np.concatenate(shifted_tca_array, axis=0)\n",
        "  windowed_tca_array = shifted_tca_array[:, t0:t1]\n",
        "  concept_arrays.append(windowed_tca_array)\n",
        "\n",
        "concept_colors = {\n",
        "    \"SINE0\": (cols[13], cols[13]),\n",
        "    \"SINE1\": (cols[9], cols[9])\n",
        "}\n",
        "\n",
        "fig = plt.figure(figsize=(6, 4), dpi=300)\n",
        "gs = gridspec.GridSpec(nrows=1, ncols=1, figure=fig)\n",
        "gs.update(wspace=0.4, hspace=0.2)\n",
        "ax = fig.add_subplot(gs[0, 0])\n",
        "set_axis_style(ax)\n",
        "ax.set_xlabel(\"Time\", fontsize=\"x-large\", wrap=True, fontweight='bold')\n",
        "ax.set_ylabel(\"tCA\", fontsize=\"x-large\", wrap=True, fontweight='bold')\n",
        "for concept_idx, concept_name in enumerate(concept_names):\n",
        "  array = concept_arrays[concept_idx]\n",
        "  plot_with_concept_mask_over_time(\n",
        "      array=array, \n",
        "      concept_mask=eval_dataset_concepts[:, dataset_concept_names.index(concept_name)], \n",
        "      ax=ax, \n",
        "      color_pair=concept_colors[concept_name],\n",
        "      concept_name=concept_name)\n",
        "ax.set_title(f\"Target y_{target}\", fontweight='bold')\n",
        "legend = ax.legend(loc=\"upper right\", title=\"Score\", bbox_to_anchor=(1.65,1.1),frameon=True)\n",
        "legend.get_title().set_fontsize(\"20\")\n",
        "legend.get_title().set_fontweight(\"bold\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XnZN_IEK9oOy"
      },
      "outputs": [],
      "source": [
        "#@title Local CS \u0026 tCA score time series example\n",
        "\n",
        "example_idx =  2#@param {type: 'integer'}\n",
        "layer = 2 #@param {type: 'integer'}\n",
        "\n",
        "label_idx_to_color = {\n",
        "    0: cols[13],\n",
        "    1: cols[9]\n",
        "}\n",
        "concept_to_color = {\n",
        "    \"SINE0\": cols[13],\n",
        "    \"SINE1\": cols[9]\n",
        "}\n",
        "\n",
        "fig = plt.figure(figsize=(12,20), dpi=300)\n",
        "gs = gridspec.GridSpec(nrows=7, ncols=1, figure=fig)\n",
        "gs.update(wspace=0.4, hspace=0.5)\n",
        "\n",
        "ax0 = fig.add_subplot(gs[0])\n",
        "for label_idx in [0, 1]:\n",
        "  ax0.plot(range(100), eval_dataset_label[:, example_idx, label_idx], label=f\"Label y_{label_idx}\", color=label_idx_to_color[label_idx], linewidth=2)\n",
        "  ax0.plot(range(100), eval_dataset_probs[:, example_idx, label_idx], color=label_idx_to_color[label_idx],marker=\"o\",linewidth=2, label=f\"Prediction y_{label_idx}\",\n",
        "           markersize=4, linestyle=\"--\")\n",
        "plt.ylim(-0.1, 1.1)\n",
        "legend = ax0.legend(loc=\"upper right\", bbox_to_anchor=(1.36, 1))\n",
        "\n",
        "ax0.set_ylabel(\"Predictions\", fontsize=\"x-large\", wrap=True, fontweight='bold')\n",
        "\n",
        "ax = None\n",
        "for target in [0, 1]:\n",
        "  for concept_idx, concept_name in enumerate(concept_names):\n",
        "    ax = fig.add_subplot(gs[concept_idx + (2 * target) + 1], sharey=ax)\n",
        "    score = cs_results[target][concept_name][layer][\"bootstrap_CS\"][:, example_idx]\n",
        "    mean = np.nanmean(score, axis=1)\n",
        "    std = np.nanstd(score, axis=1)\n",
        "    ax.plot(list(range(t0,t1)), mean, label=concept_name, color=concept_to_color[concept_name], linewidth=2)\n",
        "    ax.fill_between(list(range(t0,t1)), mean-std, mean+std, color=concept_to_color[concept_name],alpha=0.4)\n",
        "    score = cs_results[target][concept_name][layer][\"permuted_CS\"][:, example_idx]\n",
        "    mean = np.nanmean(score, axis=1)\n",
        "    std = np.nanstd(score, axis=1)\n",
        "    ax.plot(list(range(t0,t1)), mean, label=f\"Permuted\", color=[0.6, 0.6, 0.6, 0.8])\n",
        "    ax.fill_between(list(range(t0,t1)), mean-std, mean+std, color=[0.6, 0.6, 0.6, 0.8],alpha=0.1)\n",
        "    ax.legend(loc=\"upper right\",bbox_to_anchor=(1.3,1))\n",
        "    set_axis_style(ax)\n",
        "    ax.set_title(f\"CS: label y_{target}, concept={concept_name}\", fontweight=\"bold\")\n",
        "    ax.set_ylabel(\"CS\", fontsize=\"x-large\", wrap=True, fontweight=\"bold\")\n",
        "\n",
        "ax = None\n",
        "for concept_idx, concept_name in enumerate(concept_names):\n",
        "  ax = fig.add_subplot(gs[concept_idx + 5], sharey=ax)\n",
        "  score = tca_results[dummy_target][concept_name][layer][\"bootstrap_tCA\"][:, example_idx]\n",
        "  mean = np.nanmean(score, axis=1)\n",
        "  std = np.nanstd(score, axis=1)\n",
        "  ax.plot(list(range(t0,t1)), mean, label=concept_name, color=concept_to_color[concept_name], linewidth=2)\n",
        "  ax.fill_between(list(range(t0,t1)), mean-std, mean+std, color=concept_to_color[concept_name],alpha=0.4)\n",
        "  score = tca_results[dummy_target][concept_name][layer][\"permuted_tCA\"][:, example_idx]\n",
        "  mean = np.nanmean(score, axis=1)\n",
        "  std = np.nanstd(score, axis=1)\n",
        "  ax.plot(list(range(t0,t1)), mean, label=f\"Permuted\", color=[0.6, 0.6, 0.6, 0.8])\n",
        "  ax.fill_between(list(range(t0,t1)), mean-std, mean+std, color=[0.6, 0.6, 0.6, 0.8],alpha=0.1)\n",
        "  ax.legend(loc=\"upper right\",bbox_to_anchor=(1.3,1))\n",
        "  set_axis_style(ax)\n",
        "  ax.set_ylabel(\"tCA\", fontsize=\"x-large\", wrap=True, fontweight=\"bold\")\n",
        "  ax.set_title(f\"tCA: concept={concept_name}\", fontweight=\"bold\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "executionInfo": {
          "elapsed": 433,
          "status": "ok",
          "timestamp": 1606066004544,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": 300
        },
        "id": "4O945tCIzaOG"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook3",
        "kind": "private"
      },
      "name": "synthetic_dataset_figures.ipynb",
      "provenance": [
        {
          "file_id": "1Wvux4vZQtLUyCPiNPr0xrK0TXRSZArrR",
          "timestamp": 1605484076744
        },
        {
          "file_id": "1C0S3FSCCj6o-c4WxIZSxt833DOe8kBLS",
          "timestamp": 1601306411959
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
